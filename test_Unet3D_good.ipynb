{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "from config import DATA_DIR, TARGET_DIR\n",
    "import numpy as np\n",
    "\n",
    "def process_experiment_runs(base_dir):\n",
    "    # Dictionnaire pour stocker les résultats par TS\n",
    "    data_results = {}\n",
    "\n",
    "    # Parcourir tous les sous-dossiers dans ExperimentRuns\n",
    "    for ts_folder in os.listdir(base_dir):\n",
    "        ts_path = os.path.join(base_dir, ts_folder)\n",
    "\n",
    "        # Vérifier si le dossier contient un fichier Zarr\n",
    "        zarr_path = os.path.join(ts_path, \"VoxelSpacing10.000/denoised.zarr\")\n",
    "        if os.path.exists(zarr_path):\n",
    "            print(f\"Traitement de {ts_folder}...\")\n",
    "\n",
    "            # Charger le groupe Zarr\n",
    "            zgroup = zarr.open_group(zarr_path, mode='r')\n",
    "            \n",
    "            # Visualiser l'arborescence\n",
    "            print(f\"Arborescence pour {ts_folder}:\")\n",
    "            print(zgroup.tree())\n",
    "            \n",
    "            # Dictionnaire pour stocker les données de ce TS\n",
    "            ts_data = {}\n",
    "\n",
    "            # Parcourir les sous-groupes (0, 1, 2, ...)\n",
    "            for subgroup_key in zgroup.keys():\n",
    "                subgroup = zgroup[subgroup_key]\n",
    "\n",
    "                # Extraire les métadonnées et les données\n",
    "                ts_data[subgroup_key] = {\n",
    "                    \"attrs\": dict(subgroup.attrs),  # Convertir les métadonnées en dictionnaire\n",
    "                    \"info\": subgroup.info,\n",
    "                    \"data\": subgroup[:],  # Charger les données complètes\n",
    "                }\n",
    "\n",
    "            # Ajouter les données au dictionnaire global\n",
    "            data_results[ts_folder] = ts_data\n",
    "\n",
    "    return data_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Traiter les tomogrammes\n",
    "all_data = process_experiment_runs(DATA_DIR)\n",
    "\n",
    "# Sauvegarder les résultats ou continuer l'analyse\n",
    "print(\"Traitement terminé. Résumé des données extraites :\")\n",
    "for ts_name, ts_content in all_data.items():\n",
    "    print(f\"- {ts_name}: {len(ts_content)} sous-groupes traités.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['TS_5_4'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['TS_5_4']['0']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Afficher une coupe z=0 pour chaque résolution\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].imshow(all_data['TS_5_4']['0']['data'][0,:,:], cmap='gray')\n",
    "axs[0].set_title(\"Pleine résolution (184, 630, 630)\")\n",
    "axs[1].imshow(all_data['TS_5_4']['1']['data'][0,:,:], cmap='gray')\n",
    "axs[1].set_title(\"Moyenne résolution (92, 315, 315)\")\n",
    "axs[2].imshow(all_data['TS_5_4']['2']['data'][0,:,:], cmap='gray')\n",
    "axs[2].set_title(\"Basse résolution (46, 158, 158)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_targets(base_path):\n",
    "    # Dictionnaire pour stocker les informations de chaque tomogramme\n",
    "    target_data = {}\n",
    "\n",
    "    # Parcourir tous les dossiers dans le chemin de base (numéros de tomogrammes)\n",
    "    for tomogram_folder in os.listdir(base_path):\n",
    "        tomogram_path = os.path.join(base_path, tomogram_folder)\n",
    "\n",
    "        # Vérifier si un dossier \"Picks\" existe dans le tomogramme\n",
    "        picks_path = os.path.join(tomogram_path, \"Picks\")\n",
    "        if not os.path.exists(picks_path):\n",
    "            print(f\"Pas de dossier 'Picks' dans {tomogram_folder}\")\n",
    "            continue\n",
    "\n",
    "        # Parcourir tous les fichiers JSON dans le dossier \"Picks\"\n",
    "        for json_file in os.listdir(picks_path):\n",
    "            if json_file.endswith(\".json\"):\n",
    "                json_path = os.path.join(picks_path, json_file)\n",
    "\n",
    "                # Charger le fichier JSON\n",
    "                with open(json_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                # Extraire le nom de la molécule et la localisation des points\n",
    "                molecule_name = data.get(\"pickable_object_name\", \"inconnu\")\n",
    "                points = data.get(\"points\", [])\n",
    "\n",
    "                # Ajouter les informations au dictionnaire\n",
    "                if tomogram_folder not in target_data:\n",
    "                    target_data[tomogram_folder] = {}\n",
    "\n",
    "                # Sauvegarder les données pour chaque molécule dans le tomogramme\n",
    "                target_data[tomogram_folder][molecule_name] = points\n",
    "\n",
    "    return target_data\n",
    "\n",
    "# Charger les targets\n",
    "all_targets = load_targets(TARGET_DIR)\n",
    "\n",
    "# Affichage des données chargées\n",
    "print(\"Résumé des données de targets :\")\n",
    "for tomogram, molecules in all_targets.items():\n",
    "    print(f\"- {tomogram}: {len(molecules)} molécules trouvées\")\n",
    "    for molecule, points in molecules.items():\n",
    "        print(f\"  * {molecule}: {len(points)} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# --- Histogramme global ---\n",
    "global_counts = {}\n",
    "for tomogram, molecules in all_targets.items():\n",
    "    for molecule, points in molecules.items():\n",
    "        global_counts[molecule] = global_counts.get(molecule, 0) + len(points)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(global_counts.keys(), global_counts.values(), color='skyblue')\n",
    "plt.xlabel(\"Type de protéine\")\n",
    "plt.ylabel(\"Nombre d'exemplaires\")\n",
    "plt.title(\"Histogramme global du nombre d'exemplaires de protéines\")\n",
    "plt.show()\n",
    "\n",
    "# --- Histogrammes détaillés par tomogramme ---\n",
    "# Déterminer le nombre de tomogrammes à afficher\n",
    "num_tomograms = len(all_targets)\n",
    "# Choisir une disposition en grille adaptée (par exemple 2 colonnes si plus de 2 tomogrammes)\n",
    "cols = 2\n",
    "rows = math.ceil(num_tomograms / cols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(cols*6, rows*4), squeeze=False)\n",
    "\n",
    "# Parcourir les tomogrammes et tracer l'histogramme pour chacun\n",
    "tomogram_names = list(all_targets.keys())\n",
    "for idx, tomogram in enumerate(tomogram_names):\n",
    "    row = idx // cols\n",
    "    col = idx % cols\n",
    "    ax = axes[row][col]\n",
    "    \n",
    "    # Calculer le nombre d'exemplaires pour chaque protéine dans ce tomogramme\n",
    "    counts = {molecule: len(points) for molecule, points in all_targets[tomogram].items()}\n",
    "    \n",
    "    ax.bar(counts.keys(), counts.values(), color='lightgreen')\n",
    "    ax.set_title(tomogram)\n",
    "    ax.set_xlabel(\"Protéine\")\n",
    "    ax.set_ylabel(\"Nombre d'exemplaires\")\n",
    "    # Rotation des étiquettes pour une meilleure lisibilité\n",
    "    ax.tick_params(axis='x', labelrotation=45, labelsize=8)\n",
    "\n",
    "# Supprimer les sous-graphes vides s'il y en a\n",
    "total_plots = rows * cols\n",
    "if num_tomograms < total_plots:\n",
    "    for idx in range(num_tomograms, total_plots):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        fig.delaxes(axes[row][col])\n",
    "\n",
    "plt.suptitle(\"Histogrammes par tomogramme du nombre d'exemplaires de protéines\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import json\n",
    "\n",
    "def prepare_dataset(image_path, target_path):\n",
    "    \"\"\"\n",
    "    Prépare un dataset associant les données d'images aux targets (protéines et positions).\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Chemin vers le dossier contenant les images Zarr.\n",
    "        target_path (str): Chemin vers le dossier contenant les targets (fichiers JSON).\n",
    "\n",
    "    Returns:\n",
    "        list: Liste de dictionnaires, où chaque élément contient les données d'un tomogramme :\n",
    "            - \"name\": Nom du tomogramme.\n",
    "            - \"images\": Liste des résolutions (volumes 3D).\n",
    "            - \"targets\": Dictionnaire {type_molécule: [positions (x, y, z)]}.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    # Parcourir les tomogrammes dans le dossier des images\n",
    "    for tomogram_name in os.listdir(image_path):\n",
    "        tomogram_image_path = os.path.join(image_path, tomogram_name, \"VoxelSpacing10.000/denoised.zarr\")\n",
    "        tomogram_target_path = os.path.join(target_path, tomogram_name, \"Picks\")\n",
    "\n",
    "        # Vérifier que les données Zarr et les targets existent\n",
    "        if not os.path.exists(tomogram_image_path):\n",
    "            print(f\"Images non trouvées pour {tomogram_name}, ignoré.\")\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(tomogram_target_path):\n",
    "            print(f\"Targets non trouvés pour {tomogram_name}, ignoré.\")\n",
    "            continue\n",
    "\n",
    "        # Charger les images (volumes 3D à plusieurs résolutions)\n",
    "        zgroup = zarr.open_group(tomogram_image_path, mode='r')\n",
    "        sorted_keys = sorted(zgroup.keys(), key=lambda k: np.prod(zgroup[k].shape), reverse=True)\n",
    "        images = [zgroup[key][:] for key in sorted_keys]\n",
    "\n",
    "        # Charger les targets (localisations des particules)\n",
    "        targets = {}\n",
    "        for json_file in os.listdir(tomogram_target_path):\n",
    "            if json_file.endswith(\".json\"):\n",
    "                json_path = os.path.join(tomogram_target_path, json_file)\n",
    "                with open(json_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    molecule_name = data.get(\"pickable_object_name\", \"unknown\")\n",
    "                    points = [\n",
    "                        [point[\"location\"][\"x\"], point[\"location\"][\"y\"], point[\"location\"][\"z\"]]\n",
    "                        for point in data[\"points\"]\n",
    "                    ]\n",
    "                    if molecule_name not in targets:\n",
    "                        targets[molecule_name] = []\n",
    "                    targets[molecule_name].extend(points)\n",
    "\n",
    "        # Ajouter les données du tomogramme au dataset\n",
    "        dataset.append({\n",
    "            \"name\": tomogram_name,\n",
    "            \"images\": images,  # Liste des résolutions\n",
    "            \"targets\": targets  # Localisations des particules par type\n",
    "        })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# print tomogram shapes for each resolution in the order of the dataset\n",
    "def print_shapes(dataset):\n",
    "    for tomogram in dataset:\n",
    "        print(f\"Tomogramme {tomogram['name']}:\")\n",
    "        for i, image in enumerate(tomogram['images']):\n",
    "            print(f\"  - Résolution {i}: {image.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins des données\n",
    "\n",
    "# Préparer le dataset\n",
    "dataset = prepare_dataset(DATA_DIR, TARGET_DIR)\n",
    "\n",
    "print_shapes(dataset)\n",
    "\n",
    "# Exemple : Afficher les données du premier tomogramme\n",
    "print(f\"Nom du tomogramme : {dataset[0]['name']}\")\n",
    "print(f\"Forme de l'image (résolution 1) : {dataset[0]['images'][0].shape}\")\n",
    "print(f\"Targets : {dataset[0]['targets']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# On suppose que les dictionnaires all_data et all_targets ont été générés par vos fonctions process_experiment_runs et load_targets.\n",
    "# Par exemple :\n",
    "#   all_data = process_experiment_runs(DATA_DIR)\n",
    "#   all_targets = load_targets(TARGET_DIR)\n",
    "\n",
    "voxel_size = 10  # Même valeur utilisée lors du chargement et de l'analyse\n",
    "\n",
    "# Choisir un tomogramme et une résolution (ici, 'TS_5_4' et le groupe '0' pour la pleine résolution)\n",
    "tomogram_folder = 'TS_5_4'\n",
    "resolution_group = '0'\n",
    "slice_index = 90  # Coupe à afficher sur l'axe Z\n",
    "\n",
    "# Charger la coupe du tomogramme\n",
    "tomogram_volume = all_data[tomogram_folder][resolution_group]['data']\n",
    "image_slice = tomogram_volume[slice_index, :, :]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_slice, cmap='gray')\n",
    "plt.title(f\"Tomogramme {tomogram_folder} - Coupe Z={slice_index}\")\n",
    "\n",
    "# Vérifier si des cibles (targets) sont présentes pour ce tomogramme\n",
    "if tomogram_folder in all_targets:\n",
    "    molecules = all_targets[tomogram_folder]\n",
    "    \n",
    "    # Pour éviter les doublons dans la légende\n",
    "    legend_entries = {}\n",
    "    \n",
    "    for molecule, points in molecules.items():\n",
    "        for point in points:\n",
    "            # Extraction des coordonnées depuis la clé \"location\"\n",
    "            x = point[\"location\"][\"x\"] / voxel_size\n",
    "            y = point[\"location\"][\"y\"] / voxel_size\n",
    "            z = point[\"location\"][\"z\"] / voxel_size  # Coordonnée Z en voxels\n",
    "            \n",
    "            # N'afficher que les points proches de la coupe sélectionnée\n",
    "            if abs(z - slice_index) < 3:  # Seuil ajustable selon la précision désirée\n",
    "                sc = plt.scatter(x, y, s=20)\n",
    "                if molecule not in legend_entries:\n",
    "                    legend_entries[molecule] = sc\n",
    "\n",
    "    if legend_entries:\n",
    "        plt.legend(legend_entries.values(), legend_entries.keys(), loc='upper right')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same with napari\n",
    "\n",
    "colors = {\n",
    "    \"apo-ferritin\": 'red',\n",
    "    \"beta-amylase\": 'green',\n",
    "    \"beta-galactosidase\": 'blue',\n",
    "    \"ribosome\": 'orange',\n",
    "    \"thyroglobulin\": 'purple',\n",
    "    \"virus-like-particle\": 'cyan',\n",
    "    # Vous pouvez ajouter d'autres molécules si nécessaire.\n",
    "}\n",
    "\n",
    "import napari\n",
    "\n",
    "# Choisir un tomogramme et une résolution (ici, 'TS_5_4' et le groupe '0' pour la pleine résolution)\n",
    "tomogram_folder = 'TS_5_4'\n",
    "resolution_group = '0'\n",
    "\n",
    "# Charger le volume du tomogramme\n",
    "tomogram_volume = all_data[tomogram_folder][resolution_group]['data']\n",
    "\n",
    "# Créer une nouvelle instance Napari\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "# Ajouter le volume du tomogramme\n",
    "viewer.add_image(tomogram_volume, name=tomogram_folder)\n",
    "\n",
    "# Ajouter les cibles (targets) si elles existent\n",
    "if tomogram_folder in all_targets:\n",
    "    molecules = all_targets[tomogram_folder]\n",
    "    print(f\"Chargement des cibles pour {tomogram_folder}...\")\n",
    "    print('Il y a', len(molecules), 'molécules')\n",
    "    \n",
    "    for molecule, points in molecules.items():\n",
    "        print(f\"  * {molecule}: {len(points)} points\")\n",
    "        points_array = np.array([\n",
    "            [point[\"location\"][\"z\"], point[\"location\"][\"y\"], point[\"location\"][\"x\"]]  # Permuter les axes X et Z\n",
    "            for point in points\n",
    "        ]) / voxel_size\n",
    "\n",
    "        print(\"Shape du tomogramme:\", tomogram_volume.shape)\n",
    "        print(\"Min des points:\", points_array.min(axis=0))\n",
    "        print(\"Max des points:\", points_array.max(axis=0))\n",
    "\n",
    "        print(\"Il y a\", len(points_array), \"points\")\n",
    "        print(\"Premier point:\", points_array[0])\n",
    "        \n",
    "\n",
    "        # Ajouter les points à la visualisation (petite sphère pour chaque point)\n",
    "        viewer.add_points(points_array, name=molecule, size=13, face_color=colors.get(molecule, 'white'))\n",
    "        \n",
    "    # vérifier l'origine des axes (en haut à gauche dans napari)\n",
    "    viewer.add_points(np.array([[0, 0, 0]]), name='origine', size=13, face_color='yellow')\n",
    "\n",
    "\n",
    "\n",
    "# run the viewer\n",
    "viewer.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# On suppose que les dictionnaires all_data et all_targets ont été générés par vos fonctions process_experiment_runs et load_targets.\n",
    "# Par exemple :\n",
    "#   all_data = process_experiment_runs(DATA_DIR)\n",
    "#   all_targets = load_targets(TARGET_DIR)\n",
    "\n",
    "# Pour la résolution 2, le voxel_size est 4 fois celui de la résolution 0 (4 * 10 = 40)\n",
    "voxel_size = 40  \n",
    "tomogram_folder = 'TS_5_4'\n",
    "resolution_group = '2'\n",
    "slice_index = 23  # Choix d'une slice au milieu du volume de résolution 2 (le volume a 46 slices)\n",
    "\n",
    "# Charger la coupe du tomogramme à la résolution 2\n",
    "tomogram_volume = all_data[tomogram_folder][resolution_group]['data']\n",
    "image_slice = tomogram_volume[slice_index, :, :]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_slice, cmap='gray')\n",
    "plt.title(f\"Tomogramme {tomogram_folder} - Résolution 2 - Slice Z={slice_index}\")\n",
    "\n",
    "# Vérifier si des cibles (targets) sont présentes pour ce tomogramme\n",
    "if tomogram_folder in all_targets:\n",
    "    molecules = all_targets[tomogram_folder]\n",
    "    \n",
    "    # Pour éviter les doublons dans la légende\n",
    "    legend_entries = {}\n",
    "    \n",
    "    for molecule, points in molecules.items():\n",
    "        for point in points:\n",
    "            # Conversion des coordonnées physiques en indices voxels en divisant par voxel_size (40 à cette résolution)\n",
    "            x = point[\"location\"][\"x\"] / voxel_size\n",
    "            y = point[\"location\"][\"y\"] / voxel_size\n",
    "            z = point[\"location\"][\"z\"] / voxel_size  # Coordonnée Z en voxels\n",
    "            \n",
    "            # Afficher uniquement les points proches de la slice sélectionnée\n",
    "            if abs(z - slice_index) < 0.75:\n",
    "                sc = plt.scatter(x, y, s=20)\n",
    "                if molecule not in legend_entries:\n",
    "                    legend_entries[molecule] = sc\n",
    "\n",
    "    if legend_entries:\n",
    "        plt.legend(legend_entries.values(), legend_entries.keys(), loc='upper right')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARTIE 2 : PRÉPARATION DES PATCHES ET DES MASQUES DE SEGMENTATION ---\n",
    "import tensorflow as tf\n",
    "voxel_size = 10  # Même valeur utilisée précédemment\n",
    "\n",
    "# Mapping des molécules aux indices de classes.\n",
    "# La classe 0 sera réservée au fond.\n",
    "class_mapping = {\n",
    "    \"background\": 0,\n",
    "    \"apo-ferritin\": 1,\n",
    "    \"beta-amylase\": 2,\n",
    "    \"ribosome\": 3,\n",
    "    \"thyroglobulin\": 4,\n",
    "    \"virus-like-particle\": 5,\n",
    "    \"beta-galactosidase\": 6\n",
    "    # Vous pouvez ajouter d'autres molécules si nécessaire.\n",
    "}\n",
    "\n",
    "def generate_mask(volume_shape, targets, voxel_size, class_mapping, sphere_radius=2):\n",
    "    \"\"\"\n",
    "    Crée un masque de segmentation (de dimensions volume_shape) à partir des targets.\n",
    "    Pour chaque point, on convertit la position physique en indice voxel et on dessine\n",
    "    une petite sphère (de rayon sphere_radius voxels) avec la classe correspondante.\n",
    "    \n",
    "    Args:\n",
    "        volume_shape (tuple): Dimensions du volume (Z, Y, X).\n",
    "        targets (dict): Dictionnaire des targets avec pour chaque molécule une liste de points.\n",
    "        voxel_size (float): Facteur de conversion des coordonnées physiques en indices voxels.\n",
    "        class_mapping (dict): Mapping de la molécule vers l'indice de classe (0 pour le fond, >0 pour les particules).\n",
    "        sphere_radius (int): Rayon de la sphère (en voxels) à dessiner autour de chaque point.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Masque de segmentation de dimensions volume_shape.\n",
    "    \"\"\"\n",
    "    mask = np.zeros(volume_shape, dtype=np.uint8)  # Fond = 0\n",
    "    for molecule, points in targets.items():\n",
    "        if molecule not in class_mapping:\n",
    "            continue\n",
    "        class_idx = class_mapping[molecule]\n",
    "        for point in points:\n",
    "            # Si le point est un dictionnaire avec la clé \"location\", on l'utilise\n",
    "            if isinstance(point, dict) and \"location\" in point:\n",
    "                x_coord = point[\"location\"][\"x\"]\n",
    "                y_coord = point[\"location\"][\"y\"]\n",
    "                z_coord = point[\"location\"][\"z\"]\n",
    "            else:\n",
    "                # Sinon, on suppose que le point est une liste/tuple de coordonnées [x, y, z]\n",
    "                x_coord, y_coord, z_coord = point\n",
    "            \n",
    "            x_center = int(round(x_coord / voxel_size))\n",
    "            y_center = int(round(y_coord / voxel_size))\n",
    "            z_center = int(round(z_coord / voxel_size))\n",
    "            \n",
    "            # Dessiner une petite sphère autour du point\n",
    "            for dz in range(-sphere_radius, sphere_radius + 1):\n",
    "                for dy in range(-sphere_radius, sphere_radius + 1):\n",
    "                    for dx in range(-sphere_radius, sphere_radius + 1):\n",
    "                        if dx**2 + dy**2 + dz**2 <= sphere_radius**2:\n",
    "                            z_idx = z_center + dz\n",
    "                            y_idx = y_center + dy\n",
    "                            x_idx = x_center + dx\n",
    "                            if (0 <= z_idx < volume_shape[0] and\n",
    "                                0 <= y_idx < volume_shape[1] and\n",
    "                                0 <= x_idx < volume_shape[2]):\n",
    "                                mask[z_idx, y_idx, x_idx] = class_idx\n",
    "    return mask\n",
    "\n",
    "def extract_grid_patches(volume, mask, num_cubes_axis):\n",
    "    \"\"\"\n",
    "    Extrait tous les patches qui couvrent entièrement le volume en divisant chaque dimension en num_cubes_axis segments.\n",
    "    On suppose que le volume est exactement divisible par num_cubes_axis sur chaque axe.\n",
    "    \n",
    "    Args:\n",
    "        volume (ndarray): Volume 3D d'entrée, de forme (Z, Y, X).\n",
    "        mask (ndarray): Masque associé, de même forme.\n",
    "        num_cubes_axis (int): Nombre de segments (patchs) par axe (exemple : 4 donnera 4x4x4 = 64 patches).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (patches_img, patches_mask) \n",
    "               - patches_img : liste de patches d'image.\n",
    "               - patches_mask : liste de patches de masque.\n",
    "    \"\"\"\n",
    "    z_dim, y_dim, x_dim = volume.shape\n",
    "    patch_size_z = z_dim // num_cubes_axis\n",
    "    patch_size_y = y_dim // num_cubes_axis\n",
    "    patch_size_x = x_dim // num_cubes_axis\n",
    "    \n",
    "    patches_img = []\n",
    "    patches_mask = []\n",
    "    \n",
    "    for i in range(num_cubes_axis):\n",
    "        for j in range(num_cubes_axis):\n",
    "            for k in range(num_cubes_axis):\n",
    "                z0 = i * patch_size_z\n",
    "                y0 = j * patch_size_y\n",
    "                x0 = k * patch_size_x\n",
    "                patch_img = volume[z0:z0+patch_size_z, y0:y0+patch_size_y, x0:x0+patch_size_x]\n",
    "                patch_mask = mask[z0:z0+patch_size_z, y0:y0+patch_size_y, x0:x0+patch_size_x]\n",
    "                patches_img.append(patch_img)\n",
    "                patches_mask.append(patch_mask)\n",
    "    \n",
    "    return patches_img, patches_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "Y_train_list = []\n",
    "\n",
    "# Ici, num_cubes_axis doit être choisi de façon à diviser exactement les dimensions du volume.\n",
    "# Par exemple, pour le volume (184, 630, 630), on pourra choisir num_cubes_axis = 2 ou 4\n",
    "# si les dimensions sont exactement divisibles par ce nombre.\n",
    "num_cubes_axis = 2  # Exemple : divise le volume en 2x2x2 = 8 patches\n",
    "\n",
    "for tomogram in dataset:\n",
    "    volume = tomogram[\"images\"][2]  # Utilisation de la résolution la plus fine\n",
    "    mask_full = generate_mask(volume.shape, tomogram[\"targets\"], voxel_size, class_mapping)\n",
    "    print(f\"Tomogramme {tomogram['name']} volume: {volume.shape}, masque: {mask_full.shape}\")\n",
    "    \n",
    "    patches_img, patches_mask = extract_grid_patches(volume, mask_full, num_cubes_axis)\n",
    "    \n",
    "    for patch_img, patch_mask in zip(patches_img, patches_mask):\n",
    "         # On ajoute la dimension de canal à l'image patch (le modèle attend (patch_size_z, patch_size_y, patch_size_x, 1))\n",
    "         patch_img = patch_img[..., np.newaxis]\n",
    "         X_train_list.append(patch_img)\n",
    "         Y_train_list.append(patch_mask)\n",
    "\n",
    "X_train = np.array(X_train_list, dtype=np.float32)\n",
    "Y_train_int = np.array(Y_train_list, dtype=np.uint8)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train_int shape:\", Y_train_int.shape)\n",
    "\n",
    "# Conversion du masque en encodage one-hot (le modèle attend n_classes canaux en sortie)\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train_int, num_classes=7)\n",
    "print(\"Y_train shape (one-hot):\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries since execution state was reset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload Y_train_int if available\n",
    "# Assuming Y_train_int is available after the reset\n",
    "# If not, you may need to redefine and reload the dataset\n",
    "\n",
    "# Determine the presence of proteins in each patch (binary: 0 = background, 1 = contains proteins)\n",
    "patch_contains_protein = (Y_train_int > 0).any(axis=(1, 2, 3))\n",
    "\n",
    "# Compute proportions\n",
    "num_patches = len(Y_train_int)\n",
    "num_with_proteins = np.sum(patch_contains_protein)\n",
    "num_without_proteins = num_patches - num_with_proteins\n",
    "\n",
    "# Display as a pie chart\n",
    "labels = [\"Avec Protéines\", \"Sans Protéines\"]\n",
    "sizes = [num_with_proteins, num_without_proteins]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['skyblue', 'lightcoral'], startangle=90)\n",
    "plt.title(\"Proportion des Patches Contenant des Protéines\")\n",
    "plt.show()\n",
    "\n",
    "# Display numerical results\n",
    "num_with_proteins, num_without_proteins, num_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Déterminer les patchs qui contiennent des protéines (au moins un voxel avec une valeur > 0)\n",
    "patch_contains_protein = (Y_train_int > 0).any(axis=(1, 2, 3))\n",
    "\n",
    "# Séparer les indices des patchs avec et sans protéines\n",
    "indices_with_protein = np.where(patch_contains_protein)[0]\n",
    "indices_without_protein = np.where(~patch_contains_protein)[0]\n",
    "\n",
    "# Déterminer combien de patchs sans protéines nous devons garder pour équilibrer à 50/50\n",
    "num_with_protein = len(indices_with_protein)\n",
    "num_without_protein_needed = num_with_protein  # Pour obtenir un équilibre 50/50\n",
    "\n",
    "# Échantillonner aléatoirement les patchs sans protéines pour atteindre l'équilibre\n",
    "indices_without_protein_sampled = np.random.choice(indices_without_protein, num_without_protein_needed, replace=False)\n",
    "\n",
    "# Concaténer les indices des patchs sélectionnés\n",
    "balanced_indices = np.concatenate([indices_with_protein, indices_without_protein_sampled])\n",
    "\n",
    "# Mélanger les indices pour éviter tout biais d'ordre\n",
    "np.random.shuffle(balanced_indices)\n",
    "\n",
    "# Construire les nouveaux ensembles X_train et Y_train équilibrés\n",
    "X_train_balanced = X_train[balanced_indices]\n",
    "Y_train_int_balanced = Y_train_int[balanced_indices]\n",
    "\n",
    "# Conversion du masque en encodage one-hot (le modèle attend n_classes canaux en sortie)\n",
    "Y_train_balanced = tf.keras.utils.to_categorical(Y_train_int_balanced, num_classes=7)\n",
    "\n",
    "# Vérifier les nouvelles proportions\n",
    "num_patches_balanced = len(Y_train_int_balanced)\n",
    "num_with_proteins_balanced = np.sum((Y_train_int_balanced > 0).any(axis=(1, 2, 3)))\n",
    "num_without_proteins_balanced = num_patches_balanced - num_with_proteins_balanced\n",
    "\n",
    "# Affichage des proportions sous forme de camembert\n",
    "labels = [\"Avec Protéines\", \"Sans Protéines\"]\n",
    "sizes = [num_with_proteins_balanced, num_without_proteins_balanced]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['skyblue', 'lightcoral'], startangle=90)\n",
    "plt.title(\"Proportion des Patches Contenant des Protéines (Équilibré)\")\n",
    "plt.show()\n",
    "\n",
    "# Affichage des proportions des classes (protéines spécifiques)\n",
    "unique_classes, counts = np.unique(Y_train_int_balanced[Y_train_int_balanced > 0], return_counts=True)\n",
    "\n",
    "# Mapping des classes vers les noms de protéines\n",
    "class_labels = {\n",
    "    0: \"background\",\n",
    "    1: \"apo-ferritin\",\n",
    "    2: \"beta-amylase\",\n",
    "    3: \"beta-galactosidase\",\n",
    "    4: \"ribosome\",\n",
    "    5: \"thyroglobulin\",\n",
    "    6: \"virus-like-particle\",\n",
    "    7: \"beta-galactosidase\"\n",
    "}\n",
    "\n",
    "# Convertir indices de classes en noms\n",
    "labels = [class_labels.get(int(cls), f\"Classe {cls}\") for cls in unique_classes]\n",
    "sizes = counts\n",
    "\n",
    "# Affichage des proportions des protéines sous forme de camembert\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['red', 'blue', 'green', 'purple', 'orange', 'cyan'], startangle=140)\n",
    "plt.title(\"Proportion de chaque protéine dans le dataset équilibré\")\n",
    "plt.show()\n",
    "\n",
    "# Afficher les tailles finales\n",
    "X_train_balanced.shape, Y_train_balanced.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_tomogram_and_target(all_data, all_targets, tomogram_folder, resolution_group, slice_index, voxel_size, class_mapping, sphere_radius=2):\n",
    "    \"\"\"\n",
    "    Affiche côte à côte la coupe du tomogramme et le masque target associé.\n",
    "    \"\"\"\n",
    "    # Charger le volume et extraire la coupe\n",
    "    volume = all_data[tomogram_folder][resolution_group]['data']\n",
    "    image_slice = volume[slice_index, :, :]\n",
    "    \n",
    "    # Générer le masque complet à partir des targets\n",
    "    mask_full = generate_mask(volume.shape, all_targets.get(tomogram_folder, {}), voxel_size, class_mapping, sphere_radius)\n",
    "    target_slice = mask_full[slice_index, :, :]\n",
    "\n",
    "    # Création de la figure avec deux sous-graphes\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    # Palette de couleurs fixe pour chaque molécule\n",
    "    colors = {\n",
    "        \"apo-ferritin\": \"red\",\n",
    "        \"beta-amylase\": \"blue\",\n",
    "        \"ribosome\": \"purple\",\n",
    "        \"thyroglobulin\": \"orange\",\n",
    "        \"virus-like-particle\": \"cyan\",\n",
    "        \"beta-galactosidase\": \"green\"\n",
    "    }\n",
    "    \n",
    "    # --- Affichage de la coupe du tomogramme avec les targets ---\n",
    "    axs[0].imshow(image_slice, cmap='gray')\n",
    "    axs[0].set_title(f\"Tomogramme {tomogram_folder} - Slice Z={slice_index}\")\n",
    "\n",
    "    legend_entries = {}\n",
    "    targets = all_targets.get(tomogram_folder, {})\n",
    "\n",
    "    for molecule, points in targets.items():\n",
    "        color = colors.get(molecule, \"yellow\")  # Assurer une couleur par défaut si inconnue\n",
    "        for point in points:\n",
    "            x = point[\"location\"][\"x\"] / voxel_size\n",
    "            y = point[\"location\"][\"y\"] / voxel_size\n",
    "            z = point[\"location\"][\"z\"] / voxel_size\n",
    "            if abs(z - slice_index) < 1:\n",
    "                sc = axs[0].scatter(x, y, s=50, edgecolors=color, facecolors='none', linewidths=1.5)\n",
    "                if molecule not in legend_entries:\n",
    "                    legend_entries[molecule] = sc\n",
    "\n",
    "    if legend_entries:\n",
    "        axs[0].legend(legend_entries.values(), legend_entries.keys(), loc='upper right')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # --- Affichage du masque généré ---\n",
    "    axs[1].imshow(target_slice, cmap='jet')\n",
    "    axs[1].set_title(f\"Masque Target {tomogram_folder} - Slice Z={slice_index}\")\n",
    "\n",
    "    legend_entries_mask = {}\n",
    "    for molecule, points in targets.items():\n",
    "        color = colors.get(molecule, \"yellow\")\n",
    "        for point in points:\n",
    "            x = point[\"location\"][\"x\"] / voxel_size\n",
    "            y = point[\"location\"][\"y\"] / voxel_size\n",
    "            z = point[\"location\"][\"z\"] / voxel_size\n",
    "            if abs(z - slice_index) < 1:\n",
    "                sc = axs[1].scatter(x, y, s=20, edgecolors=color, facecolors='none', linewidths=1.5)\n",
    "                if molecule not in legend_entries_mask:\n",
    "                    legend_entries_mask[molecule] = sc\n",
    "\n",
    "    if legend_entries_mask:\n",
    "        axs[1].legend(legend_entries_mask.values(), legend_entries_mask.keys(), loc='upper right')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Exemple d'utilisation ---\n",
    "# On suppose que all_data et all_targets ont été générés précédemment,\n",
    "# et que class_mapping est défini (comme dans l'exemple fourni).\n",
    "voxel_size = 40\n",
    "tomogram_folder = 'TS_5_4'\n",
    "resolution_group = '2'\n",
    "slice_index = 23\n",
    "\n",
    "display_tomogram_and_target(all_data, all_targets, tomogram_folder, resolution_group, slice_index, voxel_size, class_mapping, sphere_radius=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_balanced.shape)\n",
    "print(Y_train_balanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Afficher les valeurs uniques présentes dans Y_train (données segmentées)\n",
    "unique_values = np.unique(Y_train_int)  # Y_train_int est la version non one-hot de Y_train\n",
    "\n",
    "unique_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Y_train contient NaN ?\", np.isnan(Y_train).any())\n",
    "print(\"X_train contient NaN ?\", np.isnan(X_train).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_accuracy(y_true, y_pred):\n",
    "    y_true_labels = tf.argmax(y_true, axis=-1)\n",
    "    y_pred_labels = tf.argmax(y_pred, axis=-1)\n",
    "    mask = tf.not_equal(y_true_labels, 0)\n",
    "    correct = tf.equal(y_true_labels, y_pred_labels)\n",
    "    correct_masked = tf.boolean_mask(correct, mask)\n",
    "    \n",
    "    # Si aucun voxel de protéine n'est présent, retourner 0 pour éviter NaN.\n",
    "    return tf.cond(tf.equal(tf.size(correct_masked), 0),\n",
    "                   lambda: tf.constant(0.0, dtype=tf.float32),\n",
    "                   lambda: tf.reduce_mean(tf.cast(correct_masked, tf.float32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, Cropping3D, Concatenate, Dropout, BatchNormalization, Activation, ZeroPadding3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "\n",
    "# Vos poids de classe pour les protéines (les clés correspondent aux noms de molécules)\n",
    "protein_weights = {\n",
    "    'background': 0.5,\n",
    "    'apo-ferritin': 1,\n",
    "    'beta-amylase': 0.01,\n",
    "    'beta-galactosidase': 2,\n",
    "    'ribosome': 1,\n",
    "    'thyroglobulin': 2,\n",
    "    'virus-like-particle': 1,\n",
    "}\n",
    "\n",
    "# Mapping des molécules vers leurs indices (classe 0 = fond)\n",
    "class_mapping = {\n",
    "    \"background\": 0,\n",
    "    \"apo-ferritin\": 1,\n",
    "    \"beta-amylase\": 2,\n",
    "    \"ribosome\": 3,\n",
    "    \"thyroglobulin\": 4,\n",
    "    \"virus-like-particle\": 5,\n",
    "    \"beta-galactosidase\": 6\n",
    "}\n",
    "# On suppose ici que le nombre total de classes (background + protéines) est 7.\n",
    "n_classes = 7\n",
    "\n",
    "\n",
    "@register_keras_serializable()\n",
    "def weighted_categorical_crossentropy(weights_dict, class_mapping, num_classes):\n",
    "    \"\"\"\n",
    "    Fonction de perte pondérée qui ignore le fond (classe 0).\n",
    "    \"\"\"\n",
    "    weight_vector = np.ones(num_classes, dtype=np.float32)\n",
    "    for molecule, class_idx in class_mapping.items():\n",
    "        if molecule in weights_dict:\n",
    "            weight_vector[class_idx] = weights_dict[molecule]\n",
    "    weight_vector = tf.constant(weight_vector, dtype=tf.float32)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n",
    "        loss_unweighted = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "\n",
    "        # Exclure les voxels de classe 0 (fond)\n",
    "        true_class = tf.argmax(y_true, axis=-1)\n",
    "        mask = tf.not_equal(true_class, 0)\n",
    "        voxel_weights = tf.gather(weight_vector, true_class)\n",
    "        weighted_loss = loss_unweighted * voxel_weights\n",
    "\n",
    "        # Appliquer le masque : ne considérer que les protéines\n",
    "        masked_loss = tf.boolean_mask(weighted_loss, mask)\n",
    "\n",
    "        return tf.cond(\n",
    "            tf.greater(tf.size(masked_loss), 0),  \n",
    "            lambda: tf.reduce_mean(masked_loss),  \n",
    "            lambda: tf.constant(0.0, dtype=tf.float32)\n",
    "        )\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def conv_block(inputs, n_filters, dropout=0, batch_norm=True):\n",
    "    x = Conv3D(n_filters, kernel_size=3, padding='same')(inputs)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv3D(n_filters, kernel_size=3, padding='same')(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if dropout > 0:\n",
    "        x = Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def unet3d_model(input_shape, n_classes, filters=[16, 32, 64], dropout=0):\n",
    "    inputs = Input(input_shape)\n",
    "    # Encoder\n",
    "    c1 = conv_block(inputs, filters[0], dropout)\n",
    "    p1 = MaxPooling3D(pool_size=(2,2,2), padding=\"same\")(c1)\n",
    "    \n",
    "    c2 = conv_block(p1, filters[1], dropout)\n",
    "    p2 = MaxPooling3D(pool_size=(2,2,2), padding=\"same\")(c2)\n",
    "    \n",
    "    c3 = conv_block(p2, filters[2], dropout)\n",
    "    p3 = MaxPooling3D(pool_size=(2,2,2), padding=\"same\")(c3)\n",
    "    \n",
    "    # Bottleneck\n",
    "    c4 = conv_block(p3, filters[2]*2, dropout)\n",
    "    # Decoder\n",
    "    u3 = UpSampling3D(size=(2,2,2))(c4)\n",
    "    # Ici, c3 a été obtenu avec padding=\"same\". On s'attend à ce que u3 et c3 aient les mêmes dimensions.\n",
    "    u3 = Concatenate()([u3, c3])\n",
    "    c5 = conv_block(u3, filters[2], dropout)\n",
    "    \n",
    "    u2 = UpSampling3D(size=(2,2,2))(c5)\n",
    "    u2 = Concatenate()([u2, c2])\n",
    "    c6 = conv_block(u2, filters[1], dropout)\n",
    "    \n",
    "    u1 = UpSampling3D(size=(2,2,2))(c6)\n",
    "    # On remarque que la dimension spatiale de u1 est calculée par multiplication par 2,\n",
    "    # ce qui peut donner une taille légèrement différente de celle de c1.\n",
    "    # Par exemple, avec une entrée de (23,79,79), u1 pourrait avoir (24,80,80).\n",
    "    # On applique alors un ZeroPadding3D sur c1 pour l'ajuster.\n",
    "    c1_pad = ZeroPadding3D(padding=((0,1), (0,1), (0,1)))(c1)\n",
    "    u1 = Concatenate()([u1, c1_pad])\n",
    "    c7 = conv_block(u1, filters[0], dropout)\n",
    "\n",
    "\n",
    "    # Cropping pour ajuster la taille finale\n",
    "    output_crop = Cropping3D(cropping=((0,1), (0,1), (0,1)))(c7)\n",
    "    outputs = Conv3D(n_classes, kernel_size=1, activation='softmax')(output_crop)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Paramètres d'entrée adaptés à votre cas (par exemple, avec des patches de résolution 2)\n",
    "input_shape = (23, 79, 79, 1)\n",
    "n_classes = 7\n",
    "filters = [16, 32, 64]\n",
    "dropout = 0\n",
    "\n",
    "loss_fn = weighted_categorical_crossentropy(protein_weights, class_mapping, n_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet3d_model(input_shape, n_classes, filters, dropout)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy', protein_accuracy])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Ensuite, vous pouvez lancer l'entraînement comme précédemment :\n",
    "history = model.fit(X_train, Y_train, batch_size=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle entraîné (modèle complet, y compris l'architecture, les poids et les paramètres d'optimisation et fonction de perte)\n",
    "model.save('unet3d_trained_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redéfinir la fonction de perte pour l'utiliser dans le chargement du modèle\n",
    "loss_fn = weighted_categorical_crossentropy(protein_weights, class_mapping, n_classes)\n",
    "\n",
    "# Charger le modèle avec la bonne fonction de perte et la métrique personnalisée\n",
    "model = tf.keras.models.load_model(\n",
    "    \"unet3d_trained_model.keras\",\n",
    "    custom_objects={\n",
    "        \"protein_accuracy\": protein_accuracy,\n",
    "        \"loss\": loss_fn\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Vérifier que le modèle est bien chargé\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner un patch au hasard dans X_test\n",
    "index = np.random.randint(0, len(X_test))\n",
    "\n",
    "# Sauvegarde du modèle entraîné\n",
    "model.save('unet3d_trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# 🔹 **Palette de couleurs pour les protéines**\n",
    "colors = {\n",
    "    0: \"gray\",\n",
    "    1: \"red\",\n",
    "    2: \"blue\",\n",
    "    3: \"green\",\n",
    "    4: \"purple\",\n",
    "    5: \"orange\",\n",
    "    6: \"cyan\",\n",
    "}\n",
    "\n",
    "# 🔹 **Mapping des classes vers les noms des protéines**\n",
    "class_labels = {\n",
    "    0: \"Fond\",\n",
    "    1: \"apo-ferritin\",\n",
    "    2: \"beta-amylase\",\n",
    "    3: \"ribosome\",\n",
    "    4: \"thyroglobulin\",\n",
    "    5: \"virus-like-particle\",\n",
    "    6: \"beta-galactosidase\"\n",
    "}\n",
    "\n",
    "# Sélectionner un patch au hasard dans X_train\n",
    "index = np.random.randint(0, len(X_train))\n",
    "\n",
    "# Extraire le patch d'entraînement\n",
    "x_sample = X_train[index]  # (23, 79, 79, 1)\n",
    "y_true = Y_train[index]    # (23, 79, 79, 8) -> One-hot\n",
    "\n",
    "# Ajouter une dimension batch pour la prédiction\n",
    "x_sample_expanded = np.expand_dims(x_sample, axis=0)\n",
    "\n",
    "# Effectuer la prédiction\n",
    "y_pred = model.predict(x_sample_expanded)\n",
    "\n",
    "# Convertir y_pred en indices de classes (prendre la classe avec la plus haute probabilité)\n",
    "y_pred_classes = np.argmax(y_pred[0], axis=-1)  # (23, 79, 79)\n",
    "y_true_classes = np.argmax(y_true, axis=-1)  # (23, 79, 79)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_prediction(x_sample, y_true_classes, y_pred_classes, slice_index=11):\n",
    "    \"\"\"\n",
    "    Affiche une coupe du patch testé, avec :\n",
    "    - L'image brute (X_test)\n",
    "    - Le masque réel (Y_test)\n",
    "    - La prédiction du modèle (Y_pred)\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Image brute\n",
    "    axs[0].imshow(x_sample[slice_index, :, :, 0], cmap=\"gray\")\n",
    "    axs[0].set_title(f\"Image Brute - Slice Z={slice_index}\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    # Masque de vérité terrain (Y_test)\n",
    "    axs[1].imshow(y_true_classes[slice_index, :, :], cmap=\"jet\")\n",
    "    axs[1].set_title(f\"Masque Vérité Terrain - Slice Z={slice_index}\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    # Prédiction du modèle (Y_pred)\n",
    "    axs[2].imshow(y_pred_classes[slice_index, :, :], cmap=\"jet\")\n",
    "    axs[2].set_title(f\"Prédiction Modèle - Slice Z={slice_index}\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Afficher les résultats pour une coupe Z donnée\n",
    "display_prediction(x_sample, y_true_classes, y_pred_classes, slice_index=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
